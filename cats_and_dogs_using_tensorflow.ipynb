{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cats and Dogs using the TensorFlow Backend\n",
    "\n",
    "This notebook shows how to take the 'Cats and Dogs' classifier from the very first lesson of the amazing 'Practical Deep Learning For Coders' course, and convert it to run using TensorFlow (instead of Theano).\n",
    "\n",
    "http://course.fast.ai/\n",
    "\n",
    "Why do we care what backend we use?\n",
    "\n",
    "There are a few reasons, many of which are well covered here (as well as some good instructions on installing TensorFlow):\n",
    "    http://www.pyimagesearch.com/2016/11/14/installing-keras-with-tensorflow-backend/\n",
    "\n",
    "My compeling use case is that TensorFlow has good support (and example code) for running models on Android and iOS.  However, in order to use them, we'll need to be able to export our Keras model as a TensorFlow graph (more on this later).\n",
    "\n",
    "For simplicity (and so you can inspect every step), I've taken the examples from the course notebook, and modified them so that everything fits into a single notebook without external library files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Install Tensorflow and configure it as your Keras backend\n",
    "\n",
    "Install TensorFlow on your device or VM image (personally I use the anaconda install):\n",
    "    https://www.tensorflow.org/versions/master/get_started/os_setup#anaconda_installation\n",
    "    \n",
    "Configure TensorFlow as your Keras backend:\n",
    "    https://keras.io/backend/\n",
    "    \n",
    "For reference the following is my Keras config file (you might have a real machine, and want to use GPU instead):\n",
    "```\n",
    "$ cat $HOME/.keras/keras.json \n",
    "{\n",
    "    \"image_dim_ordering\": \"tf\",\n",
    "    \"epsilon\": 1e-07,\n",
    "    \"floatx\": \"float32\",\n",
    "    \"backend\": \"tensorflow\"\n",
    "}\n",
    "```\n",
    "\n",
    "Note: That we could have saved ourselves a lot of trouble by using Theano (\"th\") dim ordering with the TensorFlow backend.  Although this is supported, I've found that training is *much* slower using Theano dim ordering, and doing the full conversion now is probably going to save us some work in the long run anyway.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Ensure we are using the Tensorflow backend.\n",
    "\n",
    "Ok, so let's do our regular imports, but make sure that Keras is using TensorFlow.\n",
    "\n",
    "The first time you do the import, Keras will tell you which backend you're using when you import the first keras module.  Make sure it's TensorFlow before going any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Define our VGG16 model\n",
    "\n",
    "The VGG16 model we define below is *almost* identical to the one from the Deep Learning Lesson 1 Notebook (https://github.com/fastai/courses/blob/master/deeplearning1/nbs/lesson1.ipynb).\n",
    "\n",
    "However, there are a few key differences:\n",
    "* Some of the syntatic sugar we used with Theano for slicing tensors does not work using TensorFlow.\n",
    "* TensorFlow orders image data in a different order from Theano (\"image_dim_ordering\": \"tf\"). Theano puts the channel data at the front of the tensor, while TensorFlow puts it at the back.\n",
    "* As you'll see later, in order to convert our existing weights we actually need our model to support both types of ordering (so we added code to support both)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_preprocess(x):\n",
    "    # We need to construct the vgg_mean differently depending on the\n",
    "    # dimention ordering.\n",
    "    if (K.image_dim_ordering() == 'th'):\n",
    "        vgg_mean = np.array([123.68, 116.779, 103.939]).reshape((3,1,1))\n",
    "        axis = 1\n",
    "    else:\n",
    "        vgg_mean = np.array([123.68, 116.779, 103.939]).reshape((1,1,3))\n",
    "        axis = 3\n",
    "    \n",
    "    x = x - vgg_mean     # subtract mean\n",
    "    \n",
    "    # TensorFlow tensors do not support this type of slice operation:\n",
    "    #     return x[:, ::-1]    # reverse axis bgr->rgb\n",
    "    # So have to split and then concatenate the channels in reverse.\n",
    "    \n",
    "    r,g,b = tf.split(x, 3, axis)\n",
    "    return tf.concat([b,g,r], axis)\n",
    "\n",
    "# Define convenience functions for the repeating blocks of VGG16\n",
    "def ConvBlock(layers, model, filters):\n",
    "    for i in range(layers): \n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "def FCBlock(model):\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "def VGG_16():\n",
    "    model = Sequential()\n",
    "    # The input dimentions differ depending on the dimention ordering.\n",
    "    if (K.image_dim_ordering() == 'th'):\n",
    "        model.add(Lambda(vgg_preprocess, input_shape=(3,224,224), output_shape=((3,224,224))))\n",
    "    else:\n",
    "        model.add(Lambda(vgg_preprocess, input_shape=(224,224,3), output_shape=((224,224,3))))\n",
    "    ConvBlock(2, model, 64)\n",
    "    ConvBlock(2, model, 128)\n",
    "    ConvBlock(3, model, 256)\n",
    "    ConvBlock(3, model, 512)\n",
    "    ConvBlock(3, model, 512)\n",
    "\n",
    "    model.add(Flatten())\n",
    "    FCBlock(model)\n",
    "    FCBlock(model)\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Convert our predefined weights\n",
    "\n",
    "This turns out to be more painful than you might think.  There are three issues we need to account for:\n",
    "* TensorFlow uses correlation for its filters, while Theano uses convolutions.  They are basically equivalent operations, but the filters are flipped (on both the x and y axis) with repect to one another.\n",
    "* The dim ordering used in our convolutional layer has changed, so we need to migrate the weights from a model defined using 'th' ordering, to one that uses 'tf' ordering.\n",
    "* We need to shuffle our dense layer weights to apply to the right outputs from the Convolutional layers.\n",
    "\n",
    "We create two models to perform the migration. One (th_model) to load the 'th' weights in, and another (tf_model) to save the 'tf' weights to.  We then load Theano Vgg16 weights for into our th_model, convert and then store them into our tf_model, and then save the weights to disk.\n",
    "\n",
    "You are free to use different models here (with your own custom weights), you just need to make sure the weights you are converting match the model.\n",
    "\n",
    "You only need to do the messy conversion once.  After this, you can just use your saved TensorFlow weights from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download the precomputed model weights for Vgg16 (in Theano format).  They are\n",
    "# too bit to store on Github\n",
    "fname = 'vgg16.h5'\n",
    "fpath = get_file(fname, 'http://www.platform.ai/models/' + fname, cache_subdir='models')\n",
    "\n",
    "# Ok, here is some messy conversion from a Theano (with 'th' dim ordering) kernel\n",
    "# to a TensorFlow (with 'tf' dim ordering) kernel.\n",
    "\n",
    "# Fortunately, Somshubra Majumdar (@titu1994) had already done a great job of figuring out how to migrate\n",
    "# between the two.  I've borrowed most of the code from the URL below:\n",
    "#   https://github.com/titu1994/Keras-Classification-Models/blob/62ab38512c6ef0fe4a0f30a00d33643f453cf12e/weight_conversion_theano.py\n",
    "\n",
    "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
    "\n",
    "first_dense = True\n",
    "nb_last_conv = 0\n",
    "\n",
    "def shuffle_rows(original_w, nb_last_conv, nb_rows_dense):\n",
    "    ''' Note :\n",
    "    This algorithm to shuffle dense layer rows was provided by Kent Sommers (@kentsommer)\n",
    "    in a gist : https://gist.github.com/kentsommer/e872f65926f1a607b94c2b464a63d0d3\n",
    "    '''\n",
    "    converted_w = np.zeros(original_w.shape)\n",
    "    count = 0\n",
    "    for index, row in enumerate(original_w):\n",
    "        if (index % nb_last_conv) == 0 and index != 0:\n",
    "            count += 1\n",
    "        new_index = ((index % nb_last_conv) * nb_rows_dense) + count\n",
    "        converted_w[new_index] = row\n",
    "    return converted_w\n",
    "\n",
    "# Create a vgg16 model with a Theano dim ordering.  You could load weights for \n",
    "# other models here, but you would need to create corresponding models.\n",
    "K.set_image_dim_ordering('th')\n",
    "th_model = VGG_16()\n",
    "# Create a vgg16 model with a Tensorflow dim ordering\n",
    "K.set_image_dim_ordering('tf')\n",
    "tf_model = VGG_16()\n",
    "\n",
    "# Load the original VGG16 model trained using the Theano backend (and 'th' dim ordering)\n",
    "th_model.load_weights(fpath)\n",
    "\n",
    "convert_all_kernels_in_model(th_model)\n",
    "\n",
    "count_dense = 0\n",
    "for layer in th_model.layers:\n",
    "    if layer.__class__.__name__ == \"Dense\":\n",
    "        count_dense += 1\n",
    "\n",
    "if count_dense == 1:\n",
    "    first_dense = False # If there is only 1 dense, no need to perform row shuffle in Dense layer\n",
    "\n",
    "for index, th_layer in enumerate(th_model.layers):\n",
    "    if th_layer.__class__.__name__ in ['Convolution1D',\n",
    "                                       'Convolution2D',\n",
    "                                       'Convolution3D',\n",
    "                                       'AtrousConvolution2D',\n",
    "                                       'Deconvolution2D']:\n",
    "        weights = th_layer.get_weights()\n",
    "        weights[0] = weights[0].transpose((2, 3, 1, 0))\n",
    "        tf_model.layers[index].set_weights(weights)\n",
    "\n",
    "        nb_last_conv = th_layer.nb_filter # preserve last number of convolutions to use with dense layers\n",
    "    else:\n",
    "        if th_layer.__class__.__name__ == \"Dense\" and first_dense:\n",
    "            weights = th_layer.get_weights()\n",
    "            nb_rows_dense_layer = weights[0].shape[0] // nb_last_conv\n",
    "\n",
    "            weights[0] = shuffle_rows(weights[0], nb_last_conv, nb_rows_dense_layer)\n",
    "            tf_model.layers[index].set_weights(weights)\n",
    "\n",
    "            first_dense = False\n",
    "        else:\n",
    "            tf_model.layers[index].set_weights(th_layer.get_weights())\n",
    "\n",
    "tf_model.save_weights('vgg16-tf.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Test\n",
    "\n",
    "Check that we can load our adjusted VGG16 weights and train against the Cats and Dogs dataset using the TensorFlow backend.\n",
    "\n",
    "We should see results that are comparble (in speed and accuracy) to using Theano backend.\n",
    "\n",
    "First we'll try the converted model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 180 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n",
      "Epoch 1/3\n",
      "180/180 [==============================] - 22s - loss: 1.0456 - acc: 0.4667 - val_loss: 0.9318 - val_acc: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/3\n",
      "180/180 [==============================] - 3s - loss: 0.9476 - acc: 0.5333 - val_loss: 0.6847 - val_acc: 0.5500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/3\n",
      "180/180 [==============================] - 3s - loss: 0.8475 - acc: 0.5389 - val_loss: 0.7330 - val_acc: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5b0b615898>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the ordering explicitly again, just to be sure.\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "# Only using sample data for this example.  You can download and use the \n",
    "# entire Cats and Dogs if you want.  It's just a bit too big for Github.\n",
    "# \n",
    "# I've tried it on the full data set, and it gets comparable accuracy to the\n",
    "# Theano backend.  You'll need try it yourself, if you don't believe me. ;)\n",
    "#\n",
    "# Update Feb 27th, 2017:  The above is a lie.  Something is broken in the\n",
    "# conversion process.\n",
    "path = \"sample/\"\n",
    "\n",
    "# As large as you can, but no larger than 64 is recommended. \n",
    "# If you have an older or cheaper GPU, you'll run out of memory, so will have to decrease this.\n",
    "batch_size=64\n",
    "\n",
    "# Create our batches for the training and validation sets\n",
    "imageGenerator = image.ImageDataGenerator()\n",
    "train_batches = imageGenerator.flow_from_directory(path+'train', target_size=(224,224),\n",
    "        class_mode='categorical', shuffle=True, batch_size=batch_size)\n",
    "imageGenerator = image.ImageDataGenerator()\n",
    "valid_batches = imageGenerator.flow_from_directory(path+'valid', target_size=(224,224),\n",
    "        class_mode='categorical', shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Test after we've done the conversion\n",
    "model = VGG_16()\n",
    "model.load_weights('vgg16-tf.h5')\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(train_batches.nb_class, activation='softmax'))\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(train_batches, samples_per_epoch=train_batches.nb_sample, \n",
    "                    validation_data=valid_batches, nb_val_samples=valid_batches.nb_sample,\n",
    "                    nb_epoch=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Now try again using the Theano dim ordering\n",
    "\n",
    "For comparison... The two should be very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 180 images belonging to 2 classes.\n",
      "Found 20 images belonging to 2 classes.\n",
      "Epoch 1/3\n",
      "180/180 [==============================] - 8s - loss: 1.7146 - acc: 0.4722 - val_loss: 0.7670 - val_acc: 0.7000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/3\n",
      "180/180 [==============================] - 7s - loss: 0.8100 - acc: 0.6500 - val_loss: 0.7266 - val_acc: 0.8000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/3\n",
      "180/180 [==============================] - 7s - loss: 0.7271 - acc: 0.7500 - val_loss: 0.8033 - val_acc: 0.8000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5b0a044160>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_image_dim_ordering('th')\n",
    "# Create our batches for the training and validation sets\n",
    "imageGenerator = image.ImageDataGenerator()\n",
    "train_batches = imageGenerator.flow_from_directory(path+'train', target_size=(224,224),\n",
    "        class_mode='categorical', shuffle=True, batch_size=batch_size)\n",
    "imageGenerator = image.ImageDataGenerator()\n",
    "valid_batches = imageGenerator.flow_from_directory(path+'valid', target_size=(224,224),\n",
    "        class_mode='categorical', shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Test after we've done the conversion\n",
    "model = VGG_16()\n",
    "model.load_weights(fpath)\n",
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False\n",
    "model.add(Dense(train_batches.nb_class, activation='softmax'))\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(train_batches, samples_per_epoch=train_batches.nb_sample, \n",
    "                    validation_data=valid_batches, nb_val_samples=valid_batches.nb_sample,\n",
    "                    nb_epoch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
